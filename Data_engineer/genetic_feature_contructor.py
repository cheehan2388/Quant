import os
import json
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import warnings
import random
from scipy.stats import pearsonr, spearmanr
warnings.filterwarnings('ignore')

def load_data(file_path='../Data/All_data/merged/BTC_merged_by_datetime.csv'):
    """Load and preprocess the merged data"""
    df = pd.read_csv(file_path)
    df['datetime'] = pd.to_datetime(df['datetime'])
    df = df.sort_values('datetime').reset_index(drop=True)
    df = df.ffill().bfill()  # Forward fill then backward fill missing values
    return df

def calculate_future_returns(df, close_dir='../Data/close', shifts=[-2, -8, -24]):
    """Calculate future returns for different shift windows"""
    future_returns = {}
    
    # Use the first close file for future returns calculation
    close_path = os.path.join(close_dir, 'Binance_1Hour_BTCUSD_T.csv')
    close_df = pd.read_csv(close_path)
    close_df['datetime'] = pd.to_datetime(close_df['datetime'])
    
    # Merge with main dataframe to get close prices
    merged = pd.merge(df, close_df[['datetime', 'Close']], on='datetime', how='left')
    
    # Calculate future returns for different shifts
    for shift in shifts:
        future_return_col = f'future_return_{abs(shift)}'
        merged[future_return_col] = merged['Close'].pct_change(periods=abs(shift)).shift(shift)
        future_returns[future_return_col] = merged[future_return_col]
    
    # Add future returns to original dataframe
    for shift in shifts:
        future_return_col = f'future_return_{abs(shift)}'
        df[future_return_col] = future_returns[future_return_col]
    
    return df, future_returns

def apply_rolling_mean(series, window):
    """Apply rolling mean transformation"""
    return series.rolling(window=window, min_periods=1).mean()

def apply_rolling_minmax_scaling(series, window):
    """Apply rolling min-max scaling"""
    def rolling_minmax(x):
        if len(x) < 2:
            return x.iloc[-1] if hasattr(x, 'iloc') else x[-1]
        min_val = x.min()
        max_val = x.max()
        if max_val == min_val:
            return 0.0
        # Return only the scaled value of the last point in the window
        return (x.iloc[-1] - min_val) / (max_val - min_val) if hasattr(x, 'iloc') else (x[-1] - min_val) / (max_val - min_val)
    
    return series.rolling(window=window, min_periods=1).apply(rolling_minmax, raw=False)

def apply_rolling_zscore(series, window):
    """Apply rolling z-score transformation"""
    def rolling_zscore(x):
        if len(x) < 2:
            return 0.0
        mean_val = x.mean()
        std_val = x.std()
        if std_val == 0:
            return 0.0
        # Return only the z-score of the last point in the window
        return (x.iloc[-1] - mean_val) / std_val if hasattr(x, 'iloc') else (x[-1] - mean_val) / std_val
    
    return series.rolling(window=window, min_periods=1).apply(rolling_zscore, raw=False)

def calculate_ic(feature_series, target_series):
    """Calculate Information Coefficient (Spearman correlation)"""
    # Remove NaN values
    valid_mask = ~(pd.isna(feature_series) | pd.isna(target_series))
    if valid_mask.sum() < 10:  # Need at least 10 valid observations
        return 0.0
    
    feature_clean = feature_series[valid_mask]
    target_clean = target_series[valid_mask]
    
    # Calculate correlation
    correlation = feature_clean.corr(target_clean, method="spearman")
    return correlation if not pd.isna(correlation) else 0.0

def generate_all_features(df):
    """Generate all transformed features according to the specification"""
    
    # Get all numeric columns (exclude datetime and future returns)
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    exclude_cols = ['datetime'] + [col for col in df.columns if 'future_return' in col]
    feature_cols = [col for col in numeric_cols if col not in exclude_cols]
    
    print(f"Processing {len(feature_cols)} base features...")
    
    # Define transformation tools
    transformations = {
        'rolling_mean_12': lambda x: apply_rolling_mean(x, 12),
        'rolling_mean_24': lambda x: apply_rolling_mean(x, 24),
        'rolling_mean_52': lambda x: apply_rolling_mean(x, 52),
        'rolling_minmax_12': lambda x: apply_rolling_minmax_scaling(x, 12),
        'rolling_minmax_24': lambda x: apply_rolling_minmax_scaling(x, 24),
        'rolling_minmax_52': lambda x: apply_rolling_minmax_scaling(x, 52),
        'rolling_zscore_12': lambda x: apply_rolling_zscore(x, 12),
        'rolling_zscore_24': lambda x: apply_rolling_zscore(x, 24),
        'rolling_zscore_52': lambda x: apply_rolling_zscore(x, 52)
    }
    
    # Store all transformed features
    transformed_features = {}
    feature_metadata = {}
    
    # Apply all transformations to all columns
    for col in feature_cols:
        print(f"Processing column: {col}")
        
        for transform_name, transform_func in transformations.items():
            feature_name = f"{col}_{transform_name}"
            try:
                transformed_series = transform_func(np.log1p(df[col]))
                transformed_features[feature_name] = transformed_series
                
                # Store metadata
                feature_metadata[feature_name] = {
                    'base_column': col,
                    'transformation': transform_name,
                    'ic_values': {},
                    'max_ic': 0.0,
                    'best_shift': None
                }
                
            except Exception as e:
                print(f"Error processing {feature_name}: {str(e)}")
                continue
    
    return transformed_features, feature_metadata

def calculate_ics_for_all_features(df, transformed_features, feature_metadata, shifts=[-2, -8, -24]):
    """Calculate IC values for all transformed features"""
    
    print(f"Calculating IC values for {len(transformed_features)} transformed features...")
    
    # Get future return columns
    future_return_cols = [f'future_return_{abs(shift)}' for shift in shifts]
    
    for feature_name, feature_series in transformed_features.items():
        ic_values = {}
        
        # Calculate IC for each shift window
        for shift in shifts:
            future_return_col = f'future_return_{abs(shift)}'
            if future_return_col in df.columns:
                ic_value = calculate_ic(feature_series, df[future_return_col])
                ic_values[f'shift_{shift}'] = ic_value
            else:
                ic_values[f'shift_{shift}'] = 0.0
        
        # Find maximum absolute IC and corresponding shift
        abs_ic_values = {k: abs(v) for k, v in ic_values.items()}
        best_shift_key = max(abs_ic_values.keys(), key=abs_ic_values.get)
        max_ic = ic_values[best_shift_key]
        
        # Update metadata
        feature_metadata[feature_name]['ic_values'] = ic_values
        feature_metadata[feature_name]['max_ic'] = max_ic
        feature_metadata[feature_name]['best_shift'] = best_shift_key
        
        # Add transformed feature to dataframe
        df[feature_name] = feature_series
    
    return df, feature_metadata

def generate_report(feature_metadata, output_path='feature_engineering_report_N_S_1.json'):
    """Generate JSON report with all feature information"""
    
    # Sort features by maximum absolute IC (descending)
    sorted_features = sorted(
        feature_metadata.items(),
        key=lambda x: abs(x[1]['max_ic']),
        reverse=True
    )
    
    report = {
        'summary': {
            'total_features': len(feature_metadata),
            'transformation_types': [
                'rolling_mean_12', 'rolling_mean_24', 'rolling_mean_52',
                'rolling_minmax_12', 'rolling_minmax_24', 'rolling_minmax_52',
                'rolling_zscore_12', 'rolling_zscore_24', 'rolling_zscore_52'
            ],
            'shift_windows': [-2, -8, -24],
            'top_10_features': [name for name, _ in sorted_features[:10]]
        },
        'features': {}
    }
    
    # Add detailed information for each feature
    for feature_name, metadata in sorted_features:
        report['features'][feature_name] = {
            'base_column': metadata['base_column'],
            'transformation': metadata['transformation'],
            'ic_values': {
                'shift_-2': metadata['ic_values'].get('shift_-2', 0.0),
                'shift_-8': metadata['ic_values'].get('shift_-8', 0.0),
                'shift_-24': metadata['ic_values'].get('shift_-24', 0.0)
            },
            'max_ic': metadata['max_ic'],
            'best_shift': metadata['best_shift'],
            'abs_max_ic': abs(metadata['max_ic'])
        }
    
    # Save report
    with open(output_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"Report saved to {output_path}")
    return report

def elite_check_function(df, feature_metadata, ic_threshold=0.05):
    """Filter and save elite features with IC > threshold"""
    print(f"\n=== ELITE FEATURE SELECTION (IC > {ic_threshold}) ===")
    
    # Filter elite features
    elite_features = {}
    elite_metadata = {}
    
    for feature_name, metadata in feature_metadata.items():
        if abs(metadata['max_ic']) > ic_threshold:
            elite_features[feature_name] = df[feature_name]
            elite_metadata[feature_name] = metadata
    
    print(f"Found {len(elite_features)} elite features out of {len(feature_metadata)} total features")
    
    if len(elite_features) == 0:
        print("No elite features found with the specified threshold.")
        return elite_features, elite_metadata
    
    # Create elite dataframe with datetime
    elite_df = pd.DataFrame({'datetime': df['datetime']})
    for feature_name, feature_series in elite_features.items():
        elite_df[feature_name] = feature_series
    
    # Save elite features to CSV
    elite_csv_path = 'elite_features.csv'
    elite_df.to_csv(elite_csv_path, index=False)
    print(f"Elite features saved to {elite_csv_path}")
    
    # Create elite JSON report
    elite_report = {
        'summary': {
            'total_elite_features': len(elite_features),
            'ic_threshold': ic_threshold,
            'average_ic': np.mean([abs(metadata['max_ic']) for metadata in elite_metadata.values()]),
            'max_ic': max([abs(metadata['max_ic']) for metadata in elite_metadata.values()]),
            'elite_feature_names': list(elite_features.keys())
        },
        'features': {}
    }
    
    # Sort elite features by absolute IC
    sorted_elite = sorted(
        elite_metadata.items(),
        key=lambda x: abs(x[1]['max_ic']),
        reverse=True
    )
    
    for feature_name, metadata in sorted_elite:
        elite_report['features'][feature_name] = {
            'base_column': metadata['base_column'],
            'transformation': metadata['transformation'],
            'ic_values': metadata['ic_values'],
            'max_ic': metadata['max_ic'],
            'best_shift': metadata['best_shift'],
            'abs_max_ic': abs(metadata['max_ic'])
        }
    
    # Save elite JSON report
    elite_json_path = 'elite_features.json'
    with open(elite_json_path, 'w') as f:
        json.dump(elite_report, f, indent=2)
    print(f"Elite features report saved to {elite_json_path}")
    
    return elite_features, elite_metadata

def split_feature_groups(df, feature_metadata, group_ic_threshold=0.01):
    """Split features into short-midterm and mid-longterm groups"""
    print(f"\n=== FEATURE GROUP SPLITTING (IC > {group_ic_threshold}) ===")
    
    short_midterm_features = {}
    mid_longterm_features = {}
    short_midterm_metadata = {}
    mid_longterm_metadata = {}
    
    for feature_name, metadata in feature_metadata.items():
        ic_values = metadata['ic_values']
        
        # Check for short-midterm: shift(-2) OR shift(-8)
        shift_2_ic = abs(ic_values.get('shift_-2', 0.0))
        shift_8_ic = abs(ic_values.get('shift_-8', 0.0))
        shift_24_ic = abs(ic_values.get('shift_-24', 0.0))
        
        # Short-midterm group: IC > threshold for shift(-2) OR shift(-8) or both
        if shift_2_ic > group_ic_threshold or shift_8_ic > group_ic_threshold:
            short_midterm_features[feature_name] = df[feature_name]
            short_midterm_metadata[feature_name] = metadata
        
        # Mid-longterm group: IC > threshold for shift(-8) OR shift(-24) or both
        if shift_8_ic > group_ic_threshold or shift_24_ic > group_ic_threshold:
            mid_longterm_features[feature_name] = df[feature_name]
            mid_longterm_metadata[feature_name] = metadata
    
    print(f"Short-Midterm Group: {len(short_midterm_features)} features")
    print(f"Mid-Longterm Group: {len(mid_longterm_features)} features")
    
    # Save short-midterm group
    if len(short_midterm_features) > 0:
        short_midterm_df = pd.DataFrame({'datetime': df['datetime']})
        for feature_name, feature_series in short_midterm_features.items():
            short_midterm_df[feature_name] = feature_series
        
        short_midterm_csv = 'short_midterm_features.csv'
        short_midterm_df.to_csv(short_midterm_csv, index=False)
        print(f"Short-midterm features saved to {short_midterm_csv}")
        
        # Short-midterm JSON report
        short_midterm_report = create_group_report(short_midterm_metadata, "Short-Midterm", group_ic_threshold)
        with open('short_midterm_features.json', 'w') as f:
            json.dump(short_midterm_report, f, indent=2)
        print("Short-midterm features report saved to short_midterm_features.json")
    
    # Save mid-longterm group
    if len(mid_longterm_features) > 0:
        mid_longterm_df = pd.DataFrame({'datetime': df['datetime']})
        for feature_name, feature_series in mid_longterm_features.items():
            mid_longterm_df[feature_name] = feature_series
        
        mid_longterm_csv = 'mid_longterm_features.csv'
        mid_longterm_df.to_csv(mid_longterm_csv, index=False)
        print(f"Mid-longterm features saved to {mid_longterm_csv}")
        
        # Mid-longterm JSON report
        mid_longterm_report = create_group_report(mid_longterm_metadata, "Mid-Longterm", group_ic_threshold)
        with open('mid_longterm_features.json', 'w') as f:
            json.dump(mid_longterm_report, f, indent=2)
        print("Mid-longterm features report saved to mid_longterm_features.json")
    
    return short_midterm_features, mid_longterm_features, short_midterm_metadata, mid_longterm_metadata

def create_group_report(metadata, group_name, threshold):
    """Create a standardized report for feature groups"""
    sorted_features = sorted(
        metadata.items(),
        key=lambda x: abs(x[1]['max_ic']),
        reverse=True
    )
    
    report = {
        'summary': {
            'group_name': group_name,
            'total_features': len(metadata),
            'ic_threshold': threshold,
            'average_ic': np.mean([abs(meta['max_ic']) for meta in metadata.values()]) if metadata else 0,
            'max_ic': max([abs(meta['max_ic']) for meta in metadata.values()]) if metadata else 0,
            'top_10_features': [name for name, _ in sorted_features[:10]]
        },
        'features': {}
    }
    
    for feature_name, meta in sorted_features:
        report['features'][feature_name] = {
            'base_column': meta['base_column'],
            'transformation': meta['transformation'],
            'ic_values': meta['ic_values'],
            'max_ic': meta['max_ic'],
            'best_shift': meta['best_shift'],
            'abs_max_ic': abs(meta['max_ic'])
        }
    
    return report

# =============================================================================
# ENHANCED GENETIC PROGRAMMING WITH GENERATION-BY-GENERATION TRACKING
# =============================================================================

class GeneticOperators:
    """Genetic programming operators for feature combination"""
    
    @staticmethod
    def safe_divide(a, b):
        """Safe division to avoid division by zero"""
        return np.where(np.abs(b) < 1e-10, np.ones_like(a), a / b)
    
    @staticmethod
    def correlation_op(a, b):
        """Calculate rolling correlation between two series"""
        # Use pandas rolling correlation
        if len(a) != len(b):
            min_len = min(len(a), len(b))
            a = a[:min_len]
            b = b[:min_len]
        
        # Create pandas series for correlation calculation
        series_a = pd.Series(a)
        series_b = pd.Series(b)
        
        # Calculate rolling correlation with window=24
        corr = series_a.rolling(window=24, min_periods=1).corr(series_b)
        return corr.fillna(0).values
    
    @staticmethod
    def apply_operator(feature1, feature2, operator):
        """Apply genetic operator to two features"""
        try:
            if operator == '+':
                return feature1 + feature2
            elif operator == '-':
                return feature1 - feature2
            elif operator == '*':
                return feature1 * feature2
            elif operator == '/':
                return GeneticOperators.safe_divide(feature1, feature2)
            elif operator == 'corr':
                return GeneticOperators.correlation_op(feature1, feature2)
            else:
                return feature1 + feature2  # Default fallback
        except Exception as e:
            print(f"Error in operator {operator}: {e}")
            return feature1 + feature2  # Safe fallback

class EnhancedGeneticFeatureEvolution:
    """Enhanced Genetic Programming with Generation Tracking"""
    
    def __init__(self, short_midterm_features, mid_longterm_features, df, 
                 operators=['+', '-', '*', '/', 'corr'], generations=12, 
                 population_size=50, mutation_rate=0.3, ic_threshold=0.05):
        self.short_midterm_features = short_midterm_features
        self.mid_longterm_features = mid_longterm_features
        self.df = df
        self.operators = operators
        self.generations = generations
        self.population_size = population_size
        self.mutation_rate = mutation_rate
        self.ic_threshold = ic_threshold
        
        # Available transformations for mutation
        self.transformations = {
            'rolling_mean_12': lambda x: apply_rolling_mean(x, 12),
            'rolling_mean_24': lambda x: apply_rolling_mean(x, 24),
            'rolling_mean_52': lambda x: apply_rolling_mean(x, 52),
            'rolling_minmax_12': lambda x: apply_rolling_minmax_scaling(x, 12),
            'rolling_minmax_24': lambda x: apply_rolling_minmax_scaling(x, 24),
            'rolling_minmax_52': lambda x: apply_rolling_minmax_scaling(x, 52),
            'rolling_zscore_12': lambda x: apply_rolling_zscore(x, 12),
            'rolling_zscore_24': lambda x: apply_rolling_zscore(x, 24),
            'rolling_zscore_52': lambda x: apply_rolling_zscore(x, 52)
        }
        
        self.population = []
        self.fitness_scores = []
        self.generation_data = []  # Store each generation's elite features
        
    def create_individual(self):
        """Create a single genetic programming individual"""
        # Choose 2 features from pools (can be from same or different pools)
        all_features = {**self.short_midterm_features, **self.mid_longterm_features}
        feature_names = list(all_features.keys())
        
        if len(feature_names) < 2:
            return None
            
        feature1_name = random.choice(feature_names)
        feature2_name = random.choice(feature_names)
        
        # Choose 1 operator
        operator = random.choice(self.operators)
        
        individual = {
            'feature1': feature1_name,
            'feature2': feature2_name,
            'operator': operator,
            'fitness': 0.0,
            'feature_data': None,
            'ic_values': {}  # Store all IC values for all shifts
        }
        
        return individual
    
    def evaluate_individual_detailed(self, individual):
        """Evaluate individual with detailed IC tracking for all shifts"""
        try:
            all_features = {**self.short_midterm_features, **self.mid_longterm_features}
            
            # Get feature data
            feature1_data = all_features[individual['feature1']].values
            feature2_data = all_features[individual['feature2']].values
            
            # Apply single operator: feature1 operator feature2
            final_feature = GeneticOperators.apply_operator(
                feature1_data, feature2_data, individual['operator']
            )
            
            # Store the evolved feature data
            individual['feature_data'] = pd.Series(final_feature, index=self.df.index)
            
            # Calculate IC against all future returns and store ALL values
            ic_values = {}
            for shift in [-2, -8, -24]:
                future_return_col = f'future_return_{abs(shift)}'
                if future_return_col in self.df.columns:
                    ic = calculate_ic(individual['feature_data'], self.df[future_return_col])
                    ic_values[f'shift_{shift}'] = ic
            
            individual['ic_values'] = ic_values
            
            # Use maximum absolute IC as fitness
            abs_ics = [abs(v) for v in ic_values.values()]
            fitness = max(abs_ics) if abs_ics else 0.0
            individual['fitness'] = fitness
            
            return fitness
            
        except Exception as e:
            print(f"Error evaluating individual: {e}")
            individual['fitness'] = 0.0
            individual['ic_values'] = {'shift_-2': 0.0, 'shift_-8': 0.0, 'shift_-24': 0.0}
            return 0.0
    
    def save_generation_elite(self, generation, elite_individuals):
        """Save elite features from current generation to CSV and JSON"""
        if not elite_individuals:
            return
        
        # Create CSV with elite features
        generation_df = pd.DataFrame({'datetime': self.df['datetime']})
        generation_metadata = {}
        
        for i, individual in enumerate(elite_individuals):
            feature_name = f"gen_{generation}_elite_{i+1}"
            
            if individual['feature_data'] is not None:
                generation_df[feature_name] = individual['feature_data']
                
                # Create detailed metadata
                generation_metadata[feature_name] = {
                    'generation': generation,
                    'rank': i + 1,
                    'fitness': individual['fitness'],
                    'feature1': individual['feature1'],
                    'feature2': individual['feature2'],
                    'operator': individual['operator'],
                    'expression': f"{individual['feature1']} {individual['operator']} {individual['feature2']}",
                    'ic_values': {
                        'shift_-2': individual['ic_values'].get('shift_-2', 0.0),
                        'shift_-8': individual['ic_values'].get('shift_-8', 0.0),
                        'shift_-24': individual['ic_values'].get('shift_-24', 0.0)
                    },
                    'best_shift': f"shift_{[k for k, v in individual['ic_values'].items() if abs(v) == individual['fitness']][0].split('_')[1]}" if individual['ic_values'] else 'none'
                }
        
        # Save generation CSV
        csv_filename = f"generation_{generation}_elite_features.csv"
        generation_df.to_csv(csv_filename, index=False)
        print(f"  📁 Generation {generation} elite features saved to {csv_filename}")
        
        # Save generation JSON
        generation_report = {
            'generation_info': {
                'generation_number': generation,
                'elite_count': len(elite_individuals),
                'ic_threshold': self.ic_threshold,
                'king_fitness': elite_individuals[0]['fitness'] if elite_individuals else 0,
                'avg_elite_fitness': np.mean([ind['fitness'] for ind in elite_individuals])
            },
            'king_of_generation': {
                'feature_name': f"gen_{generation}_elite_1",
                'fitness': elite_individuals[0]['fitness'],
                'expression': generation_metadata[f"gen_{generation}_elite_1"]['expression'],
                'ic_values': generation_metadata[f"gen_{generation}_elite_1"]['ic_values']
            } if elite_individuals else {},
            'all_elite_features': generation_metadata
        }
        
        json_filename = f"generation_{generation}_elite_report.json"
        with open(json_filename, 'w') as f:
            json.dump(generation_report, f, indent=2)
        print(f"  📊 Generation {generation} report saved to {json_filename}")
        
        # Store for overall tracking
        self.generation_data.append({
            'generation': generation,
            'elite_individuals': elite_individuals,
            'metadata': generation_metadata,
            'csv_file': csv_filename,
            'json_file': json_filename
        })
    
    def evolve_with_tracking(self):
        """Run evolution with detailed generation tracking"""
        print(f"\n🧬 ENHANCED GENETIC PROGRAMMING with Generation Tracking")
        print(f"Saving all elite features (IC > {self.ic_threshold}) from each generation")
        print("="*80)
        
        # Initialize population
        print(f"Initializing population of {self.population_size} individuals...")
        self.population = []
        
        for _ in range(self.population_size):
            individual = self.create_individual()
            if individual is not None:
                self.population.append(individual)
        
        print(f"Created {len(self.population)} individuals")
        
        if len(self.population) == 0:
            print("❌ Failed to create initial population")
            return []
        
        for generation in range(self.generations):
            print(f"\n🔬 Generation {generation + 1}/{self.generations}")
            
            # Evaluate population with detailed tracking
            valid_individuals = []
            for individual in self.population:
                fitness = self.evaluate_individual_detailed(individual)
                if fitness > self.ic_threshold:
                    valid_individuals.append(individual)
            
            # Sort by fitness (descending)
            valid_individuals.sort(key=lambda x: x['fitness'], reverse=True)
            
            print(f"📊 Elite individuals (IC > {self.ic_threshold}): {len(valid_individuals)}")
            
            if valid_individuals:
                king_fitness = valid_individuals[0]['fitness']
                avg_fitness = np.mean([ind['fitness'] for ind in valid_individuals])
                print(f"👑 King fitness: {king_fitness:.6f}")
                print(f"📈 Average elite fitness: {avg_fitness:.6f}")
                
                # Save this generation's elite features
                self.save_generation_elite(generation + 1, valid_individuals)
            else:
                print("⚠️  No elite individuals in this generation")
                # Still save empty generation for completeness
                self.save_generation_elite(generation + 1, [])
            
            # Continue evolution if not last generation
            if generation < self.generations - 1:
                # Selection and reproduction
                if len(valid_individuals) >= 2:
                    selected = valid_individuals  # Use all valid individuals
                    next_generation = []
                    
                    # Perform 5 crossover operations as requested
                    for _ in range(5):
                        parent1 = random.choice(selected)
                        parent2 = random.choice(selected)
                        
                        child1 = self.crossover(parent1, parent2)
                        child2 = self.crossover(parent2, parent1)
                        
                        # Apply mutation
                        self.mutate(child1)
                        self.mutate(child2)
                        
                        next_generation.extend([child1, child2])
                    
                    # Add elite individuals (best 20)
                    elite_size = min(20, len(selected))
                    elite = selected[:elite_size]
                    next_generation.extend([ind.copy() for ind in elite])
                    
                    # Fill remaining slots with new random individuals
                    while len(next_generation) < self.population_size:
                        new_individual = self.create_individual()
                        if new_individual is not None:
                            next_generation.append(new_individual)
                    
                    self.population = next_generation[:self.population_size]
                else:
                    # Reinitialize if no valid individuals
                    print("🔄 Reinitializing population...")
                    self.population = []
                    for _ in range(self.population_size):
                        individual = self.create_individual()
                        if individual is not None:
                            self.population.append(individual)
        
        print(f"\n✅ Evolution with tracking completed!")
        return self.generation_data
    
    def crossover(self, parent1, parent2):
        """Create offspring by combining parents"""
        child = parent1.copy()
        
        # Randomly inherit features and operator from parents
        if random.random() < 0.5:
            child['feature1'] = parent2['feature1']
        if random.random() < 0.5:
            child['feature2'] = parent2['feature2']
        if random.random() < 0.5:
            child['operator'] = parent2['operator']
        
        child['fitness'] = 0.0
        child['feature_data'] = None
        child['ic_values'] = {}
        
        return child
    
    def mutate(self, individual):
        """Apply mutation to an individual"""
        if random.random() < self.mutation_rate:
            # Choose what to mutate
            mutation_type = 'transform' #random.choice(['feature1', 'feature2', 'operator', 'transform'])
            
            # if mutation_type == 'feature1' or mutation_type == 'feature2':
            #     # Mutate feature selection
            #     all_features = {**self.short_midterm_features, **self.mid_longterm_features}
            #     feature_names = list(all_features.keys())
            #     individual[mutation_type] = random.choice(feature_names)
                
            # elif mutation_type == 'operator':
            #     # Mutate operator
            #     individual['operator'] = random.choice(self.operators)
                
            # elif mutation_type == 'transform':
                # Apply transformation to current feature data
            if individual['feature_data'] is not None:
                transform_name = random.choice(list(self.transformations.keys()))
                transform_func = self.transformations[transform_name]
                try:
                    individual['feature_data'] = transform_func(individual['feature_data'])
                except:
                    pass  # Skip if transformation fails
            
            individual['fitness'] = 0.0  # Reset fitness after mutation
            individual['ic_values'] = {}

def run_enhanced_genetic_programming(df, short_midterm_features, mid_longterm_features, 
                                   generations=12, population_size=50, mutation_rate=0.3, ic_threshold=0.05):
    """Run enhanced genetic programming with generation tracking"""
    
    if len(short_midterm_features) == 0 and len(mid_longterm_features) == 0:
        print("⚠️  No features available for genetic programming")
        return []
    
    print(f"\n🧬 ENHANCED GENETIC PROGRAMMING EVOLUTION")
    print(f"Short-midterm pool: {len(short_midterm_features)} features")
    print(f"Mid-longterm pool: {len(mid_longterm_features)} features")
    print(f"Parameters: {generations} generations, {population_size} population, {mutation_rate:.1%} mutation rate")
    print(f"Elite threshold: IC > {ic_threshold}")
    
    # Initialize enhanced genetic programming
    gp_evolution = EnhancedGeneticFeatureEvolution(
        short_midterm_features=short_midterm_features,
        mid_longterm_features=mid_longterm_features,
        df=df,
        generations=generations,
        population_size=population_size,
        mutation_rate=mutation_rate,
        ic_threshold=ic_threshold
    )
    
    # Run evolution with tracking
    generation_data = gp_evolution.evolve_with_tracking()
    
    # Create comprehensive summary
    summary_report = {
        'evolution_summary': {
            'total_generations': generations,
            'population_size': population_size,
            'mutation_rate': mutation_rate,
            'ic_threshold': ic_threshold,
            'short_midterm_pool_size': len(short_midterm_features),
            'mid_longterm_pool_size': len(mid_longterm_features)
        },
        'generation_summaries': [],
        'all_kings': [],
        'evolution_files_generated': []
    }
    
    # Process each generation's data
    for gen_data in generation_data:
        generation = gen_data['generation']
        elite_individuals = gen_data['elite_individuals']
        
        if elite_individuals:
            king = elite_individuals[0]
            summary_report['generation_summaries'].append({
                'generation': generation,
                'elite_count': len(elite_individuals),
                'king_fitness': king['fitness'],
                'avg_fitness': np.mean([ind['fitness'] for ind in elite_individuals])
            })
            
            summary_report['all_kings'].append({
                'generation': generation,
                'fitness': king['fitness'],
                'expression': f"{king['feature1']} {king['operator']} {king['feature2']}",
                'ic_values': king['ic_values']
            })
        
        summary_report['evolution_files_generated'].extend([
            gen_data['csv_file'],
            gen_data['json_file']
        ])
    
    # Save comprehensive summary
    with open('genetic_programming_evolution_summary.json', 'w') as f:
        json.dump(summary_report, f, indent=2)
    print(f"\nEvolution summary saved to genetic_programming_evolution_summary.json")
    
    return generation_data

def save_transformed_features(df, output_path='transformed_features_V_S_2.csv'):
    """Save all transformed features to CSV"""
    # Select columns to save (exclude temporary future return columns if needed)
    df.to_csv(output_path, index=False)
    print(f"Transformed features saved to {output_path}")

def main():
    """Main pipeline execution with enhanced genetic programming"""
    print("=== ENHANCED FEATURE ENGINEERING PIPELINE WITH GENETIC PROGRAMMING ===")
    
    # Step 1: Load data
    print("\n1. Loading data...")
    df = load_data()
    print(f"Loaded data with shape: {df.shape}")
    
    # Step 2: Calculate future returns
    print("\n2. Calculating future returns...")
    df, future_returns = calculate_future_returns(df)
    
    # Step 3: Generate all transformed features
    print("\n3. Applying transformations to all features...")
    transformed_features, feature_metadata = generate_all_features(df)
    
    # Step 4: Calculate IC values for all features
    print("\n4. Calculating Information Coefficients...")
    df, feature_metadata = calculate_ics_for_all_features(df, transformed_features, feature_metadata)
    
    # Step 5: Generate main report
    print("\n5. Generating main report...")
    report = generate_report(feature_metadata)
    
    # Step 6: Elite feature selection (IC > 0.05)
    print("\n6. Elite feature selection...")
    elite_features, elite_metadata = elite_check_function(df, feature_metadata, ic_threshold=0.05)
    
    # Step 7: Split features into groups
    print("\n7. Splitting features into groups...")
    short_midterm_features, mid_longterm_features, short_midterm_metadata, mid_longterm_metadata = split_feature_groups(
        df, feature_metadata, group_ic_threshold=0.01
    )
    
    # Step 8: Enhanced Genetic Programming Evolution with Generation Tracking
    print("\n8. Enhanced Genetic Programming Evolution with Generation Tracking...")
    generation_data = run_enhanced_genetic_programming(
        df=df,
        short_midterm_features=short_midterm_features,
        mid_longterm_features=mid_longterm_features,
        generations=12,
        population_size=50,
        mutation_rate=0.3,
        ic_threshold=0.05
    )
    
    # Step 9: Save all results
    print("\n9. Saving all results...")
    save_transformed_features(df)
    
    # Display comprehensive summary
    print("\n" + "="*80)
    print("COMPREHENSIVE FEATURE ANALYSIS SUMMARY WITH GENETIC PROGRAMMING")
    print("="*80)
    
    # Overall statistics
    print(f"\n📊 OVERALL STATISTICS:")
    print(f"Total features generated: {len(feature_metadata)}")
    print(f"Elite features (IC > 0.05): {len(elite_features)}")
    print(f"Short-midterm features (IC > 0.01): {len(short_midterm_features)}")
    print(f"Mid-longterm features (IC > 0.01): {len(mid_longterm_features)}")
    print(f"🧬 GP Generations processed: {len(generation_data)}")
    
    # Show GP evolution summary
    if generation_data:
        print(f"\n🧬 GENETIC PROGRAMMING EVOLUTION SUMMARY:")
        total_elite_across_generations = sum(len(gen['elite_individuals']) for gen in generation_data)
        print(f"Total elite features across all generations: {total_elite_across_generations}")
        
        # Show top 3 generations
        generations_by_king = sorted(generation_data, 
                                   key=lambda x: x['elite_individuals'][0]['fitness'] if x['elite_individuals'] else 0, 
                                   reverse=True)[:3]
        
        print(f"🏆 TOP 3 GENERATIONS BY KING FITNESS:")
        for i, gen_data in enumerate(generations_by_king, 1):
            if gen_data['elite_individuals']:
                king = gen_data['elite_individuals'][0]
                print(f"{i}. Generation {gen_data['generation']}")
                print(f"   King Fitness: {king['fitness']:.6f}")
                print(f"   Expression: {king['feature1']} {king['operator']} {king['feature2']}")
                print(f"   IC Values: {king['ic_values']}")
        
        print(f"\n📁 GENETIC PROGRAMMING FILES GENERATED:")
        print(f"  - genetic_programming_evolution_summary.json: Complete evolution overview")
        for gen_data in generation_data:
            print(f"  - {gen_data['csv_file']}: Generation {gen_data['generation']} elite features data")
            print(f"  - {gen_data['json_file']}: Generation {gen_data['generation']} detailed report")
    
    # Files generated
    print(f"\n📁 ALL FILES GENERATED:")
    print("Main outputs:")
    print("  - feature_engineering_report_N_S_1.json: Complete feature analysis")
    print("  - transformed_features_V_S_2.csv: All transformed features")
    
    if len(elite_features) > 0:
        print("Elite features:")
        print("  - elite_features.csv: Elite features data")
        print("  - elite_features.json: Elite features report")
    
    if len(short_midterm_features) > 0:
        print("Short-midterm group:")
        print("  - short_midterm_features.csv: Short-midterm features data")
        print("  - short_midterm_features.json: Short-midterm features report")
    
    if len(mid_longterm_features) > 0:
        print("Mid-longterm group:")
        print("  - mid_longterm_features.csv: Mid-longterm features data")
        print("  - mid_longterm_features.json: Mid-longterm features report")
    
    print("\n✅ Enhanced pipeline with generation tracking completed successfully!")
    print("="*80)

if __name__ == "__main__":
    main()